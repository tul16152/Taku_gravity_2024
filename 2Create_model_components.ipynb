{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model Components\n",
    "Creat the distance bands and normalised distance from glacier edge tif files used in inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import RegularGridInterpolator, interp1d, RectBivariateSpline\n",
    "# import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray\n",
    "# import fiona\n",
    "# import rasterio.mask\n",
    "import pandas as pd\n",
    "# import verde as vd\n",
    "import geopandas as gpd\n",
    "import time\n",
    "# from shapely.geometry import *\n",
    "# import rasterio as rs\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "# import pyvista as pv\n",
    "# import harmonica as hm\n",
    "from shapely import geometry\n",
    "\n",
    "#from discretize.utils import mkvc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the data measurement locations\n",
    "profile4 = pd.read_csv('Data/prof4_meas.csv')\n",
    "profile7a = pd.read_csv('Data/prof7a_meas.csv')\n",
    "longa = pd.read_csv('Data/longa_meas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_raster(file): #create a function to read in the dem in tif format and convert it to an xarray which is used by harmonica\n",
    "    #note function returns a dataArray and a Dataset which are different things - we use the dataArray - and also an array of x,y,z values\n",
    "    geotiff_da = rioxarray.open_rasterio(file)\n",
    "    geotiff_ds = geotiff_da.to_dataset('band') # Covert our xarray.DataArray into a xarray.Dataset\n",
    "    geotiff_ds = geotiff_ds.rename({1: 'topo'}) # Rename the variable to a more useful name in the dataset\n",
    "\n",
    "    geo_tiff_topo = geotiff_da[0] # in the dataArray select just the first variable which is the topography\n",
    "\n",
    "    [x_topo, y_topo] = np.meshgrid(geo_tiff_topo.x.values, geo_tiff_topo.y.values) # the x and y values are read in as a single array for each and we want to create a grid of all values\n",
    "    z_topo = geo_tiff_topo.values # defining the z component as the topography\n",
    "\n",
    "    topo_xyz = np.c_[x_topo.ravel(), y_topo.ravel(), z_topo.ravel()] # we combine the x,y and z information into one array to be returned as a standard array\n",
    "\n",
    "    topo_xr = xr.DataArray(geo_tiff_topo.values, # create the DataArray\n",
    "    coords={'y': y_topo[:,0],'x': x_topo[0,:]}, \n",
    "    dims=[\"y\", \"x\"])\n",
    "\n",
    "    return topo_xr #returns the dataset, numpy array and dataArray\n",
    "\n",
    "surf_xr = open_raster('Data/arctic_dem_plus_10m_nad83.tif')\n",
    "bed_xr = open_raster('Data/Farinotti_2019/taku_plus_bed_nad83.tif')\n",
    "bed_xr = bed_xr.reindex(y=bed_xr.y[::-1]) #invert y axis so it is negative to positive\n",
    "\n",
    "model_shape = gpd.read_file('Model_components/model_domain_nad83.shp') #load in shapefile of glacier shape for the area of measurements\n",
    "model_geom = geometry.shape(model_shape['geometry'].values[0])\n",
    "\n",
    "dist_xr = open_raster('Model_components/taku_edge_dist_nad83.tif')\n",
    "center_points = pd.read_csv('Model_components/center_points_250m_nad83.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample the data onto same sized grids\n",
    "interp = RegularGridInterpolator((surf_xr.y.values, surf_xr.x.values), surf_xr.values) # create an interpolator function\n",
    "dem_xx = np.linspace(min(bed_xr.x.values), max(bed_xr.x.values), int(len(bed_xr.x.values))) # create an array of x values that we want to resample onto - in this case I just halfed the number of values\n",
    "dem_yy = np.linspace(min(bed_xr.y.values), max(bed_xr.y.values), int(len(bed_xr.y.values)))\n",
    "xx, yy = np.meshgrid(dem_xx, dem_yy) # mesh grid the 1D arrays\n",
    "bb = interp((yy, xx)) # use the interpolator function on these new x and y positions\n",
    "\n",
    "#create a new DataArray\n",
    "surf_xr_resamp = xr.DataArray(bb,\n",
    "coords={'y': dem_yy,'x': dem_xx}, \n",
    "dims=[\"y\", \"x\"])\n",
    "\n",
    "thick_xr = surf_xr_resamp - bed_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create different xarrays for bed inside and outside model domain\n",
    "bed_outside = bed_xr.copy()\n",
    "bed_inside = bed_xr.copy()\n",
    "\n",
    "#for bed outside, assign surface elevation within model domain so ice thickness there is 0 - this is what we solve for\n",
    "xx, yy = np.meshgrid(bed_outside.x.values, bed_outside.y.values)\n",
    "a = np.array([geometry.Point(x, y) for x, y in zip(xx.ravel(), yy.ravel())], dtype=object)\n",
    "mask_geom1 = np.array([model_geom.contains(point) for point in a])\n",
    "mask_geom1 =  mask_geom1.reshape(len(bed_outside.y), len(bed_outside.x))\n",
    "bed_outside = bed_outside.where(mask_geom1 == False, surf_xr_resamp)\n",
    "mask_outside = (bed_outside == bed_outside.min().values).values\n",
    "bed_outside = bed_outside.where(mask_outside == False, surf_xr_resamp)\n",
    "\n",
    "#cut down size of bed inside to be model domain size plus 1km buffer\n",
    "model_shape_bounds = model_shape.total_bounds\n",
    "topo_buffer = 1000\n",
    "region = (model_shape_bounds[0] - topo_buffer, model_shape_bounds[2] + topo_buffer, model_shape_bounds[1] - topo_buffer, model_shape_bounds[3] + topo_buffer )\n",
    "bed_inside = bed_inside.sel(y=slice(*region[2:]), x=slice(*region[:2]))\n",
    "#cut down surface xarray to same area to be surf inside\n",
    "surf_inside = surf_xr_resamp.copy()\n",
    "surf_inside = surf_inside.sel(y=slice(*region[2:]), x=slice(*region[:2]))\n",
    "#create mask around model domain\n",
    "xx, yy = np.meshgrid(bed_inside.x.values, bed_inside.y.values)\n",
    "a = np.array([geometry.Point(x, y) for x, y in zip(xx.ravel(), yy.ravel())], dtype=object)\n",
    "mask_geom_small = np.array([model_geom.contains(point) for point in a])\n",
    "mask_geom_small =  mask_geom_small.reshape(xx.shape)\n",
    "bed_inside = bed_inside.where(mask_geom_small == True, surf_inside)\n",
    "#create mask to exclude rock areas that may be in the model domain\n",
    "mask_rock_small = (bed_inside == bed_inside.min().values).values\n",
    "bed_inside = bed_inside.where(mask_rock_small == False, surf_inside)\n",
    "thick_inside = surf_inside.copy() - bed_inside.copy()\n",
    "#set thickness to 0 outside model domain\n",
    "thick_inside = thick_inside.where(mask_geom_small == True, 0)\n",
    "thick_inside = thick_inside.where(mask_rock_small == False, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resample dist xr to size of grid being used for inversion\n",
    "interp = RegularGridInterpolator((dist_xr.y.values, dist_xr.x.values), dist_xr.values) # create an interpolator function\n",
    "dem_xx = np.linspace(min(bed_inside.x.values), max(bed_inside.x.values), int(len(bed_inside.x.values))) # create an array of x values that we want to resample onto - in this case I just halfed the number of values\n",
    "dem_yy = np.linspace(min(bed_inside.y.values), max(bed_inside.y.values), int(len(bed_inside.y.values)))\n",
    "xx, yy = np.meshgrid(dem_xx, dem_yy) # mesh grid the 1D arrays\n",
    "bb = interp((yy, xx)) # use the interpolator function on these new x and y positions\n",
    "\n",
    "#create a new DataArray\n",
    "dist_xr_resamp = xr.DataArray(bb,\n",
    "coords={'y': dem_yy,'x': dem_xx}, \n",
    "dims=[\"y\", \"x\"])\n",
    "\n",
    "dist_xr_resamp = dist_xr_resamp.where(mask_geom_small == True, -99)\n",
    "#set to nodata value on any intersecting rock\n",
    "dist_xr_resamp = dist_xr_resamp.where(mask_rock_small == False, -99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define band_xr as distance upstream of each point\n",
    "\n",
    "band_xr = dist_xr_resamp.copy()\n",
    "all_x = xx.ravel()\n",
    "all_y = yy.ravel()\n",
    "for i in range(len(all_x)):\n",
    "    center_points['dist_p'] = np.sqrt((center_points.xcoord - all_x[i])**2 + (center_points.ycoord - all_y[i])**2)\n",
    "    dist_sorted = center_points.sort_values(by='dist_p').reset_index(drop=True)\n",
    "    dist_small = dist_sorted['distance'].iloc[0]\n",
    "    band_xr.loc[dict(x=all_x[i], y=all_y[i])] = dist_small\n",
    "\n",
    "band_xr = band_xr.where(mask_geom_small == True, -900)\n",
    "band_xr = band_xr.where(mask_rock_small == False, -900)\n",
    "\n",
    "# dist_xr_buf = proxim_xr_resamp.copy()\n",
    "# for i in range(len(all_x)):\n",
    "#     dist_points_buf['dist_p'] = np.sqrt((dist_points_buf.xcoord - all_x[i])**2 + (dist_points_buf.ycoord - all_y[i])**2)\n",
    "#     dist_sorted = dist_points_buf.sort_values(by='dist_p').reset_index(drop=True)\n",
    "#     dist_small = dist_sorted['distance'].iloc[0]\n",
    "#     dist_xr_buf.loc[dict(x=all_x[i], y=all_y[i])] = dist_small\n",
    "\n",
    "# dist_xr_buf = dist_xr_buf.where(mask_geom_small == True, -900)\n",
    "# dist_xr_buf = dist_xr_buf.where(mask_rock_small == False, -900)\n",
    "\n",
    "# dist_50m_xr = proxim_xr_resamp.copy()\n",
    "# for i in range(len(all_x)):\n",
    "#     dist_points_50m['dist_p'] = np.sqrt((dist_points_50m.xcoord - all_x[i])**2 + (dist_points_50m.ycoord - all_y[i])**2)\n",
    "#     dist_sorted = dist_points_50m.sort_values(by='dist_p').reset_index(drop=True)\n",
    "#     dist_small = dist_sorted['distance'].iloc[0]\n",
    "#     dist_50m_xr.loc[dict(x=all_x[i], y=all_y[i])] = dist_small\n",
    "\n",
    "# dist_50m_xr = dist_50m_xr.where(mask_geom_small == True, -900)\n",
    "# dist_50m_xr = dist_50m_xr.where(mask_rock_small == False, -900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6g/r2y8czh94x50dtz0hyff18nr0000gq/T/ipykernel_23236/2528913802.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  center_points['dist_max'].iloc[i] = dist_xr_resamp.where(band_xr == center_points['distance'].iloc[i]).max().values\n"
     ]
    }
   ],
   "source": [
    "# normalise the distance from glacier edge in each band, for the bands around the curve which don't \n",
    "# extend the full distance normalise by the max value from the band and one each side\n",
    "center_points['dist_max'] = -999\n",
    "dist_norm_xr = dist_xr_resamp.copy()\n",
    "for i in range(len(center_points)):\n",
    "    band_i = center_points['distance'].iloc[i]\n",
    "    if band_i > 4500 and band_i < 7500:\n",
    "        max_vals = [dist_xr_resamp.where(band_xr == center_points['distance'].iloc[i]).max().values]\n",
    "        for j in range(-1,2,1):\n",
    "            max_vals.append(dist_xr_resamp.where(band_xr == center_points['distance'].iloc[i+j]).max().values)\n",
    "        center_points['dist_max'].iloc[i] = max(max_vals)\n",
    "    else:\n",
    "        center_points['dist_max'].iloc[i] = dist_xr_resamp.where(band_xr == center_points['distance'].iloc[i]).max().values\n",
    "\n",
    "    dist_norm_xr = dist_norm_xr.where(band_xr != band_i, dist_xr_resamp/center_points['dist_max'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create inside/outside array\n",
    "mod_xr = dist_xr_resamp.copy()\n",
    "for i in range(len(dist_xr_resamp)):\n",
    "    for j in range(len(dist_xr_resamp[i])):\n",
    "        #outside glacier\n",
    "        if dist_xr_resamp[i][j] < 0:\n",
    "            mod_xr[i][j] = 0\n",
    "        #inside glacier\n",
    "        else:\n",
    "            mod_xr[i][j] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export model arrays\n",
    "surf_inside.rio.write_crs(26908, inplace=True)\n",
    "surf_inside.rio.to_raster('Model_components/surf_inside.tif')\n",
    "\n",
    "bed_outside.rio.write_crs(26908, inplace=True)\n",
    "bed_outside.rio.to_raster('Model_components/bed_outside.tif')\n",
    "\n",
    "surf_xr_resamp.rio.write_crs(26908, inplace=True)\n",
    "surf_xr_resamp.rio.to_raster('Model_components/surf_xr_resamp.tif')\n",
    "\n",
    "np.savetxt('Model_components/mask_geom_small.csv', mask_geom_small, delimiter=',')\n",
    "np.savetxt('Model_components/mask_rock_small.csv', mask_rock_small, delimiter=',')\n",
    "\n",
    "band_xr.rio.write_crs(26908, inplace=True)\n",
    "band_xr.rio.to_raster('Model_components/band_xr.tif')\n",
    "\n",
    "dist_norm_xr.rio.write_crs(26908, inplace=True)\n",
    "dist_norm_xr.rio.to_raster('Model_components/dist_norm_xr.tif')\n",
    "\n",
    "mod_xr.rio.write_crs(26908, inplace=True)\n",
    "mod_xr.rio.to_raster('Model_components/mod_xr.tif')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5e680212d2c5842032ddd0bb4aa6aeee0e7da853b3ca34712facc6e4cee8989c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('simpeg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
