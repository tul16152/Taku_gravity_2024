{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Processing of Gravity Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required packages - \n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import *\n",
    "import pandas as pd\n",
    "import pyproj\n",
    "import rioxarray\n",
    "import harmonica as hm\n",
    "import xarray as xr\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "import sys\n",
    "sys.path.insert(0,'Required_packages')\n",
    "from tidegravity import *\n",
    "from scipy.interpolate import LinearNDInterpolator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data\n",
    "We define a function to read in the text file that is output by the gravity meter and then take the columns we want and put them in a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a function to read in the text tile that is output but the gravity meter \n",
    "#- you don't need to understand exactly what it does but it basically reads the the file line by line and extracts certain bits for different data columns\n",
    "linen=[]\n",
    "station=[] \n",
    "alt=[]\n",
    "grav=[]  \n",
    "sd=[]\n",
    "tiltx=[]\n",
    "tilty=[]\n",
    "temp=[]\n",
    "etc=[]\n",
    "dur=[]\n",
    "rej=[]\n",
    "t=[]\n",
    "ts = []\n",
    "corr_g=[]\n",
    "keepdata=[]\n",
    "epoch = []\n",
    "keepitem=int\n",
    "def readRawDataFile(filename):  \n",
    "        \"\"\" read a raw ascii data text file extracted from CG5\n",
    "     \n",
    "        sometimes bad values are written by the CG5 soft, if such case happen,\n",
    "        an error is raised and the user is asked for checking the data file \n",
    "        manually\n",
    "        \"\"\"    \n",
    "   \n",
    "        try:\n",
    "            #essaye d'ouvrir le fichier\n",
    "            fh = open(filename, 'r')\n",
    "            i=0\n",
    "            #PBAR = ProgressBar(total=len([1 for line in  open(filename, 'r')]),textmess='Load raw data')   \n",
    "            #print \"number of lines: %d\"%len([1 for line in  open(filename, 'r')])\n",
    "            #PBAR.show()\n",
    "            for line in fh:    \n",
    "                #PBAR.progressbar.setValue(i)\n",
    "                i+=1\n",
    "                # Clean line\n",
    "                line = line.strip()\n",
    "                # Skip blank and comment lines\n",
    "                if (not line) or (line[0] == '/') or (line[0] == 'L'): continue\n",
    "        \t     #parse string line first with respect to '/' caracters (used in the date format), \n",
    "        \t     #then with ':' (used for the time display), eventually with the classic ' '\n",
    "                vals_temp1=line.split('/')\n",
    "                vals_temp2=vals_temp1[0].split(':')\n",
    "                vals_temp3=vals_temp2[0].split()                \n",
    "                vals_temp4=vals_temp2[2].split()\n",
    "\n",
    "                # fill object properties:\n",
    "                linen.append(float(vals_temp3[0]))\n",
    "                station.append(float(vals_temp3[1]))\n",
    "                alt.append(float(vals_temp3[2]))\n",
    "                grav.append(float(vals_temp3[3]))\n",
    "                sd.append(float(vals_temp3[4]))\n",
    "                tiltx.append(float(vals_temp3[5]))\n",
    "                tilty.append(float(vals_temp3[6]))\n",
    "                temp.append(float(vals_temp3[7]))\n",
    "                etc.append(float(vals_temp3[8]))\n",
    "                dur.append(int(vals_temp3[9]))\n",
    "                rej.append(int(vals_temp3[10]))\n",
    "                t.append(datetime(int(vals_temp4[3]),int(vals_temp1[1]),\\\n",
    "                int(vals_temp1[2]),int(vals_temp3[11]),int(vals_temp2[1]),\\\n",
    "                int(vals_temp4[0])))       \n",
    "                t_tmp = '%04d-%02d-%02dT%02d:%02d:%02d'%\\\n",
    "                    (int(vals_temp4[3]),int(vals_temp1[1]),\\\n",
    "                    int(vals_temp1[2]),int(vals_temp3[11]),int(vals_temp2[1]),\\\n",
    "                    int(vals_temp4[0]))  \n",
    "                ts.append(pd.Timestamp(t_tmp))\n",
    "                epoch.append((pd.Timestamp(t_tmp) - pd.Timestamp(\"1970-01-01\")).total_seconds())\n",
    "\n",
    "                keepdata.append(1)                                                                                         \n",
    "        except IOError:\n",
    "            #si Ã§a ne marche pas, affiche ce message et continue le prog\n",
    "            print ('No file : %s' %(filename))            \n",
    "        except ValueError:\n",
    "            print ('pb at line %d : check raw data file'%(i))\n",
    "        except IndexError:\n",
    "            print ('pb at line %d : check raw data file: possibly last line?'%(i) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in gravity data\n",
    "readRawDataFile('Data/taku_grav_data.TXT') #since we have defined the function readRawDataFile in the above cell we can just call it here with our file name\n",
    "#note that if you run this multiple times it will keep adding onto the bottom of the lists - so you'll end up multiple copys of the data in your dataframe\n",
    "\n",
    "scintrex_data = pd.DataFrame({'grav':grav,'sta':station, 'timestamp':ts, 'sd':sd, 'tiltx':tiltx, 'tilty':tilty, 'temp':temp}) #here we define our pandas dataframe with the data columns we are interested in\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in and assign gps data\n",
    "\n",
    "Our gps data is stored seperately so we also want to read that in and assign the correct positions and elevations to the stations by name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6g/r2y8czh94x50dtz0hyff18nr0000gq/T/ipykernel_65358/2273284778.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  scintrex_data['x_8N'].iloc[scintrex_data.sta == stations_gps.Name.iloc[i]] = stations_gps['x_8N'].iloc[i]\n",
      "/var/folders/6g/r2y8czh94x50dtz0hyff18nr0000gq/T/ipykernel_65358/2273284778.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  scintrex_data['y_8N'].iloc[scintrex_data.sta == stations_gps.Name.iloc[i]] = stations_gps['y_8N'].iloc[i]\n",
      "/var/folders/6g/r2y8czh94x50dtz0hyff18nr0000gq/T/ipykernel_65358/2273284778.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  scintrex_data['Elev'].iloc[scintrex_data.sta == stations_gps.Name.iloc[i]] = stations_gps['Elevation'].iloc[i]\n",
      "/var/folders/6g/r2y8czh94x50dtz0hyff18nr0000gq/T/ipykernel_65358/2273284778.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  scintrex_data['Elev_std'].iloc[scintrex_data.sta == stations_gps.Name.iloc[i]] = stations_gps['Elev_std'].iloc[i]\n"
     ]
    }
   ],
   "source": [
    "#read in gps data\n",
    "stations_gps = pd.read_csv('Data/grav_all_ppk_stations.csv', usecols=[0,3,4,5,6],skiprows=1,names=['Name', 'Elev_std', 'x_8N', 'y_8N', 'Elevation'])\n",
    "#create columns with value -999 that we will overwrite with the correct value\n",
    "scintrex_data['x_8N'] = -999\n",
    "scintrex_data['y_8N'] = -999\n",
    "scintrex_data['Elev'] = -999\n",
    "scintrex_data['Elev_std'] = -999\n",
    "for i in range(len(stations_gps)): #loop through all the stations in the gps data\n",
    "    scintrex_data['x_8N'].iloc[scintrex_data.sta == stations_gps.Name.iloc[i]] = stations_gps['x_8N'].iloc[i]\n",
    "    scintrex_data['y_8N'].iloc[scintrex_data.sta == stations_gps.Name.iloc[i]] = stations_gps['y_8N'].iloc[i]\n",
    "    scintrex_data['Elev'].iloc[scintrex_data.sta == stations_gps.Name.iloc[i]] = stations_gps['Elevation'].iloc[i]\n",
    "    scintrex_data['Elev_std'].iloc[scintrex_data.sta == stations_gps.Name.iloc[i]] = stations_gps['Elev_std'].iloc[i]\n",
    "\n",
    "#at bases position is calculated from PPP processing of rinex file - we take the mean of the PPP position on multiple days\n",
    "#camp 10 was station numbers between 100 and 120\n",
    "scintrex_data['x_8N'].iloc[(scintrex_data['sta']>=100) & (scintrex_data['sta']<120)] = np.mean(stations_gps[(stations_gps['Name']>=100) & (stations_gps['Name']<120)].x_8N)\n",
    "scintrex_data['y_8N'].iloc[(scintrex_data['sta']>=100) & (scintrex_data['sta']<120)] = np.mean(stations_gps[(stations_gps['Name']>=100) & (stations_gps['Name']<120)].y_8N)\n",
    "scintrex_data['Elev'].iloc[(scintrex_data['sta']>=100) & (scintrex_data['sta']<120)] = np.mean(stations_gps[(stations_gps['Name']>=100) & (stations_gps['Name']<120)].Elevation)\n",
    "scintrex_data['Elev_std'].iloc[(scintrex_data['sta']>=100) & (scintrex_data['sta']<120)] = np.mean(stations_gps[(stations_gps['Name']>=100) & (stations_gps['Name']<120)].Elev_std)\n",
    "\n",
    "#transform to wgs84 lat long\n",
    "#define transformations\n",
    "wgs84 = 'epsg:4326' # Global lat-lon coordinate system - to be used for tide and latitude corrections\n",
    "utm8N = 'epsg:26908' #utm8n - nad 83 - the projected coordinate system being used\n",
    "utm8N2wgs = pyproj.Transformer.from_crs(utm8N, wgs84)\n",
    "scintrex_data['Lat'], scintrex_data['Long'] = utm8N2wgs.transform(scintrex_data['x_8N'].values, scintrex_data['y_8N'].values)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Earth tide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tide_calc = solve_longman_tide(scintrex_data.Lat.values, scintrex_data.Long.values, scintrex_data.Elev.values, pd.DatetimeIndex(scintrex_data.timestamp.values))\n",
    "scintrex_data['tide_calc'] = tide_calc[2]\n",
    "scintrex_data['tide_corr_grav'] = scintrex_data['grav'] + scintrex_data['tide_calc'] #we add the calculated tide correction to get the tide corrected gravity\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Measurements\n",
    "We have at least 4 measurements at each location, so we want to average over them so we just have one value at each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_names = scintrex_data['sta'].unique()\n",
    "station_avgs = pd.DataFrame(st_names, columns=['sta'])\n",
    "\n",
    "#create lists for values to extract from data\n",
    "grav_avg = []\n",
    "grav_std = []\n",
    "center_dt = []\n",
    "lat = []\n",
    "long = []\n",
    "x_8N = []\n",
    "y_8N = []\n",
    "elev = []\n",
    "elev_std = []\n",
    "\n",
    "for n in st_names:\n",
    "    #grav_vals = []\n",
    "    select = scintrex_data[scintrex_data.sta == n]\n",
    "    grav_avg.append(np.mean(select.tide_corr_grav))\n",
    "    grav_std.append(np.std(select.tide_corr_grav))\n",
    "    center_dt.append(select.timestamp.iloc[0] + (select.timestamp.iloc[-1] - select.timestamp.iloc[0])/2)\n",
    "    lat.append(np.mean(select.Lat))\n",
    "    long.append(np.mean(select.Long))\n",
    "    x_8N.append(np.mean(select.x_8N))\n",
    "    y_8N.append(np.mean(select.y_8N))\n",
    "    elev.append(np.mean(select.Elev))\n",
    "    elev_std.append(np.mean(select.Elev_std))\n",
    "#create new dataframe with all extracted values\n",
    "data_avgs = pd.DataFrame(zip(st_names, grav_avg, grav_std, center_dt, lat, long, x_8N, y_8N, elev, elev_std), columns=['sta','grav_avg', 'grav_std', 'Datetime', 'Lat', 'Long', 'x_8N', 'y_8N','Elev', 'Elev_std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we split the data from the base station to calculate the drift\n",
    "data_avgs['secs_elapsed'] = (data_avgs['Datetime'] - data_avgs['Datetime'].iloc[0])/pd.Timedelta(seconds=1)\n",
    "\n",
    "c10_base = data_avgs.loc[(data_avgs['sta']>=100) & (data_avgs['sta']<120)].reset_index(drop=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drift \n",
    "We need to calculate the effect of the drift of the gravity meter on the measurements - correct with linear relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = np.polyfit(c10_base.secs_elapsed, c10_base.grav_avg, 1)\n",
    "c10_base['linear_drift_corr'] = c10_base.grav_avg - (c10_base.secs_elapsed*a +b)\n",
    "data_avgs['linear_drift_corr'] = data_avgs.grav_avg - (data_avgs.secs_elapsed*a +b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6g/r2y8czh94x50dtz0hyff18nr0000gq/T/ipykernel_65358/130087068.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stations['linear_drift_corr'].iloc[0] = np.mean(c10_base.linear_drift_corr) #set c10 to be mean of all drift corrected measurements there\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stations = data_avgs.loc[(data_avgs['sta']<100) | (data_avgs['sta']>=200) | (data_avgs['sta']==102)].reset_index(drop=True) #stations all non base locations plus c10 at start of day 1 - part of profile 4\n",
    "stations['linear_drift_corr'].iloc[0] = np.mean(c10_base.linear_drift_corr) #set c10 to be mean of all drift corrected measurements there\n",
    "stations['grav_anom'] = stations['linear_drift_corr'] - np.mean(c10_base.linear_drift_corr) #make all measurements relative to mean at c10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latitude correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations['theo_grav_0'] =  9.7803267715*(1 + 0.0052790414*np.sin(np.deg2rad(stations.Lat))**2 \n",
    "                                            + 0.0000232718*np.sin(2*np.deg2rad(stations.Lat))**4\n",
    "                                            + 0.0000001262*np.sin(2*np.deg2rad(stations.Lat))**6\n",
    "                                            + 0.0000000007*np.sin(2*np.deg2rad(stations.Lat))**8)*1e5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error on drift corrected data - root sum of squares combination of station and base standard deviations\n",
    "#base std is the standard deviation of the C10 base station measurments after linear drift correction\n",
    "base_std = np.std(c10_base.linear_drift_corr)\n",
    "stations['drift_corr_grav_uc'] = np.sqrt(stations['grav_std']**2 + base_std**2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free air anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations['fac'] = stations.Elev * 0.3086 #( R. E. Sheriff, Encyclopedic Dictionary of Exploration Geophysics, 2nd ed., (Society of Exploration Geophysicists, 1984), p. 141.)\n",
    "stations['faa_error'] = np.sqrt((stations.Elev_std * -0.3086)**2 + (stations.drift_corr_grav_uc)**2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bouguer anomaly\n",
    "This is done as a simple slab correction with an additional component from the terrain correction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bouguer Slab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations['boug_slab_corr_2700'] = 2*np.pi*2700*stations.Elev*6.67e-11*1e5\n",
    "stations['ba_error'] = np.sqrt((0.196*stations.Elev_std)**2  + (stations.drift_corr_grav_uc)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Terrain Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the tif file of the arctic dem surface elevations and regrid at 100 m spacing \n",
    "def open_raster(file): #create a function to read in the dem in tif format and convert it to an xarray which is used by harmonica\n",
    "    #note function returns a dataArray and a Dataset which are different things - we use the dataArray - and also an array of x,y,z values\n",
    "    geotiff_da = rioxarray.open_rasterio(file)\n",
    "    geotiff_ds = geotiff_da.to_dataset('band') # Covert our xarray.DataArray into a xarray.Dataset\n",
    "    geotiff_ds = geotiff_ds.rename({1: 'topo'}) # Rename the variable to a more useful name in the dataset\n",
    "\n",
    "    geo_tiff_topo = geotiff_da[0] # in the dataArray select just the first variable which is the topography\n",
    "\n",
    "    [x_topo, y_topo] = np.meshgrid(geo_tiff_topo.x.values, geo_tiff_topo.y.values) # the x and y values are read in as a single array for each and we want to create a grid of all values\n",
    "    z_topo = geo_tiff_topo.values # defining the z component as the topography\n",
    "\n",
    "    topo_xyz = np.c_[x_topo.ravel(), y_topo.ravel(), z_topo.ravel()] # we combine the x,y and z information into one array to be returned as a standard array\n",
    "\n",
    "    topo_xr = xr.DataArray(geo_tiff_topo.values, # create the DataArray\n",
    "    coords={'y': y_topo[:,0],'x': x_topo[0,:]}, \n",
    "    dims=[\"y\", \"x\"])\n",
    "\n",
    "    return topo_xr #returns the dataset, numpy array and dataArray\n",
    "\n",
    "\n",
    "surf_xr = open_raster('Data/arctic_dem_plus_10m_nad83.tif')\n",
    "surf_xr = surf_xr.reindex(y=surf_xr.y[::-1]) #invert y axis so it is negative to positive\n",
    "mask = (surf_xr < 0).values\n",
    "surf_xr = surf_xr.where(mask == False, 0)\n",
    "\n",
    "interp = RegularGridInterpolator((surf_xr.y.values, surf_xr.x.values), surf_xr.values) # create an interpolator function\n",
    "dem_xx = np.linspace(min(surf_xr.x.values), max(surf_xr.x.values), int(len(surf_xr.x.values)/10)) # create an array of x values that we want to resample onto - in this case I just halfed the number of values\n",
    "dem_yy = np.linspace(min(surf_xr.y.values), max(surf_xr.y.values), int(len(surf_xr.y.values)/10))\n",
    "xx, yy = np.meshgrid(dem_xx, dem_yy) # mesh grid the 1D arrays\n",
    "bb = interp((yy, xx)) # use the interpolator function on these new x and y positions\n",
    "\n",
    "#create a new DataArray\n",
    "surf_xr_resamp = xr.DataArray(bb,\n",
    "coords={'y': dem_yy,'x': dem_xx}, \n",
    "dims=[\"y\", \"x\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_coords_sta = xr.DataArray(stations['x_8N']) #turn the x and y into dataArrays so they can be used to sample the dem\n",
    "y_coords_sta = xr.DataArray(stations['y_8N'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function for calculating gravity from density and surface elevation distribution\n",
    "def prism_calc(density, surface):\n",
    "    density_xr = surface.copy() # create a density data array\n",
    "    density_xr.values[:] = density  # replace every value for the density of the topography\n",
    "    prisms = hm.prism_layer( # this is where we create the model of the topography, by creating a layer of prisms with varying elevation based on the dem\n",
    "        (surface.x.values, surface.y.values),\n",
    "        surface=surface,\n",
    "        reference=0,\n",
    "        properties={\"density\": density_xr})\n",
    "    \n",
    "    dem_elev_sta = surface.sel(x=x_coords_sta, y=y_coords_sta, method='nearest') #find the dem elevation at the station and base locations\n",
    "\n",
    "    prisms_g_sta = prisms.prism_layer.gravity((stations['x_8N'], stations['y_8N'], dem_elev_sta.values), field=\"g_z\") #calculate the 3D gravity from the dem\n",
    "    terr_corr_sta = dem_elev_sta.values*2*np.pi*density*6.67e-11*1e5 - prisms_g_sta #subtract from the bouguer slab correction with the dem values\n",
    "\n",
    "    return terr_corr_sta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate uncertainty on terrain correction - take ~15 minutes, if want values from prior run uncomment first line in next cell\n",
    "n_runs = 100\n",
    "terrain_corrs_sta = np.empty((n_runs, len(stations)))\n",
    "\n",
    "for i in range(n_runs):\n",
    "    perturb = s = np.random.normal(0, 1, len(surf_xr_resamp.y)*len(surf_xr_resamp.x)) #perturb dem with standard deviation 1m\n",
    "    perturb_reshape = perturb.reshape(len(surf_xr_resamp.y), len(surf_xr_resamp.x)) \n",
    "    surf_pert = surf_xr_resamp.copy() + perturb_reshape #add pertubation\n",
    "    den = np.random.randint(2650, 2751) #select density from distribution of reasonable values\n",
    "    terrain_corrs_sta[i,:] = prism_calc(den, surf_pert) #calculate terrain correction with density and perturbed dem\n",
    "terr_corr_error = np.std(terrain_corrs_sta, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# terr_corr_error = np.array([0.06907342, 0.02967226, 0.02910847, 0.02806514, 0.02995369,\n",
    "#        0.02823702, 0.02887071, 0.03164564, 0.03570411, 0.03164564,\n",
    "#        0.02967875, 0.03207385, 0.02876628, 0.02830362, 0.02848493,\n",
    "#        0.02963774, 0.03362601, 0.03179451, 0.02889714, 0.02994969,\n",
    "#        0.03133625, 0.03121335, 0.08700415, 0.03176602, 0.0382349 ,\n",
    "#        0.03196948, 0.03279301, 0.03528296, 0.03990372, 0.03336947,\n",
    "#        0.05482707, 0.03443134, 0.03338897, 0.03501866, 0.03617349,\n",
    "#        0.03608689, 0.0373075 , 0.03293801, 0.06750231, 0.04261558,\n",
    "#        0.03498449, 0.07284373, 0.07414116, 0.0732809 , 0.03932213,\n",
    "#        0.04617974, 0.03267181, 0.0379076 , 0.0337315 , 0.04876516])\n",
    "stations['ba_terr_error'] = np.sqrt(stations['ba_error']**2 + terr_corr_error**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations['terr_corr_2700'], c10_base['terr_corr_2700']  = prism_calc(2700, surf_xr_resamp) #calculate terrain correction with density and perturbed dem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations['fa_grav_theo'] = -stations['theo_grav_0'] + stations['fac']\n",
    "stations['fa_grav_theo_rel'] = stations['fa_grav_theo'] - stations['fa_grav_theo'].iloc[0]\n",
    "stations['fa_anom'] = stations['grav_anom'] + stations['fa_grav_theo_rel']\n",
    "\n",
    "stations['grav_theo_2700'] = -stations['theo_grav_0'] + stations['fac'] - stations['boug_slab_corr_2700'] + stations['terr_corr_2700']\n",
    "stations['grav_theo_rel'] = stations['grav_theo_2700'] - stations['grav_theo_2700'].iloc[0]\n",
    "stations['boug_anom_2700'] = stations['grav_anom'] + stations['grav_theo_rel']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat measurements - error estimation and averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6g/r2y8czh94x50dtz0hyff18nr0000gq/T/ipykernel_65358/1622556540.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_rep_avg['boug_anom_2700'][data_rep_avg['sta']==rep_meas[i]] =  (data_rep_avg['boug_anom_2700'][data_rep_avg['sta']==rep_meas[i]].values + data_rep_avg['boug_anom_2700'][data_rep_avg['sta']==rep_meas[i+1]].values)/2\n"
     ]
    }
   ],
   "source": [
    "# repeat measurments differences\n",
    "rep_meas = [1, 11, 7, 71, 4, 21, 210, 212, 305, 409, 403, 408] #numbers of repeat measurement stations\n",
    "\n",
    "diff_max = stations['boug_anom_2700'][stations['sta']==rep_meas[0]].values - stations['boug_anom_2700'][stations['sta']==rep_meas[1]].values #maximum repeat measurement\n",
    "stations['rep_error_max'] = diff_max[0]\n",
    "\n",
    "data_rep_avg = stations.copy() #create new dataframe to average repeat measurements\n",
    "for i in range(0,len(rep_meas),2):\n",
    "    # data_rep_avg['faa'][data_rep_avg['sta']==rep_meas[i]] =  (data_rep_avg['faa'][data_rep_avg['sta']==rep_meas[i]].values + data_rep_avg['faa'][data_rep_avg['sta']==rep_meas[i+1]].values)/2\n",
    "    data_rep_avg['boug_anom_2700'][data_rep_avg['sta']==rep_meas[i]] =  (data_rep_avg['boug_anom_2700'][data_rep_avg['sta']==rep_meas[i]].values + data_rep_avg['boug_anom_2700'][data_rep_avg['sta']==rep_meas[i+1]].values)/2\n",
    "    data_rep_avg = data_rep_avg.drop(index = data_rep_avg[data_rep_avg['sta']==rep_meas[i+1]].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rss the measurement error to the repeat meas error\n",
    "data_rep_avg['error'] = np.sqrt(data_rep_avg['rep_error_max']**2 + data_rep_avg['ba_terr_error']**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate out Profiles\n",
    "Create seperate dataframes for each of the Profiles and calculate distance along them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6g/r2y8czh94x50dtz0hyff18nr0000gq/T/ipykernel_65358/3450042324.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  profile4['dist'].iloc[i] = profile4['dist'].iloc[i-1] + np.sqrt((profile4['x_8N'].iloc[i] - profile4['x_8N'].iloc[i-1])**2 +(profile4['y_8N'].iloc[i] - profile4['y_8N'].iloc[i-1])**2 )\n",
      "/var/folders/6g/r2y8czh94x50dtz0hyff18nr0000gq/T/ipykernel_65358/3450042324.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  longa['dist'].iloc[i] = longa['dist'].iloc[i-1] + np.sqrt((longa['x_8N'].iloc[i] - longa['x_8N'].iloc[i-1])**2 +(longa['y_8N'].iloc[i] - longa['y_8N'].iloc[i-1])**2 )\n",
      "/var/folders/6g/r2y8czh94x50dtz0hyff18nr0000gq/T/ipykernel_65358/3450042324.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  profile7a['dist'].iloc[i] = profile7a['dist'].iloc[i-1] + np.sqrt((profile7a['x_8N'].iloc[i] - profile7a['x_8N'].iloc[i-1])**2 +(profile7a['y_8N'].iloc[i] - profile7a['y_8N'].iloc[i-1])**2 )\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#seperate out the profile 4 data - day 1 station numbers plus c10 base station\n",
    "profile4 = data_rep_avg[(data_rep_avg.sta < 20) | (data_rep_avg.sta == 102)].copy()\n",
    "profile4 = profile4.sort_values(by='x_8N', ascending=False).reset_index(drop=True)\n",
    "profile4['dist'] = 0\n",
    "for i in range(1, len(profile4)):\n",
    "    profile4['dist'].iloc[i] = profile4['dist'].iloc[i-1] + np.sqrt((profile4['x_8N'].iloc[i] - profile4['x_8N'].iloc[i-1])**2 +(profile4['y_8N'].iloc[i] - profile4['y_8N'].iloc[i-1])**2 )\n",
    "\n",
    "#seperate longitudinal profile - day 2 and 3 measurements plus station 4 which is at intersection with profile 4\n",
    "longa = data_rep_avg[(data_rep_avg.sta > 19) & (data_rep_avg.sta < 99) | (data_rep_avg.sta == 4) | (data_rep_avg.sta > 199) & (data_rep_avg.sta < 399)].copy()\n",
    "longa = longa.sort_values(by='y_8N', ascending=True).reset_index(drop=True)\n",
    "longa['dist'] = 0\n",
    "for i in range(1, len(longa)):\n",
    "    longa['dist'].iloc[i] = longa['dist'].iloc[i-1] + np.sqrt((longa['x_8N'].iloc[i] - longa['x_8N'].iloc[i-1])**2 +(longa['y_8N'].iloc[i] - longa['y_8N'].iloc[i-1])**2 )\n",
    "\n",
    "#seperate profile 7a - day 4 measurments plus 305 which is where long A intersects\n",
    "profile7a = data_rep_avg[(data_rep_avg.sta> 399) | (data_rep_avg.sta == 305)].copy()\n",
    "profile7a = profile7a.sort_values(by='x_8N', ascending=False).reset_index(drop=True)\n",
    "profile7a['dist'] = 0\n",
    "for i in range(1, len(profile7a)):\n",
    "    profile7a['dist'].iloc[i] = profile7a['dist'].iloc[i-1] + np.sqrt((profile7a['x_8N'].iloc[i] - profile7a['x_8N'].iloc[i-1])**2 +(profile7a['y_8N'].iloc[i] - profile7a['y_8N'].iloc[i-1])**2 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export profiles\n",
    "profile4.to_csv('Data/prof4_meas.csv', index=False)\n",
    "profile7a.to_csv('Data/prof7a_meas.csv', index=False)\n",
    "longa.to_csv('Data/longa_meas.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
